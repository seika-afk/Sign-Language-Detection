{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o4BRH39762Ci"
      },
      "outputs": [],
      "source": [
        "!mkdir ~p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download ayuraj/asl-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEsuyblp7p5l",
        "outputId": "a4f1cc94-f1ba-42a4-c322-77b4ae12e282"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/ayuraj/asl-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading asl-dataset.zip to /content\n",
            "  0% 0.00/56.9M [00:00<?, ?B/s]\n",
            "100% 56.9M/56.9M [00:00<00:00, 1.31GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/asl-dataset.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "Wpvdi69d8Lh9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/asl_dataset/asl_dataset\n"
      ],
      "metadata": {
        "id": "xY1Xqn2J8UoO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/asl_dataset\"\n",
        "\n"
      ],
      "metadata": {
        "id": "_MYPktPt8fTB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Dataset"
      ],
      "metadata": {
        "id": "hiR8txAl94mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "image_paths=[]\n",
        "labels=[]\n",
        "\n",
        "for folder in os.listdir(path):\n",
        "  for image in os.listdir(os.path.join(path,folder)):\n",
        "    image_paths.append(os.path.join(path,folder,image))\n",
        "    labels.append(folder)\n",
        "\n",
        "data={'image_path':image_paths,'Label':labels}\n",
        "df=pd.DataFrame(data)\n",
        "df.to_csv('dataset.csv',index=False)\n",
        "print(\"trained csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6SbxGdf94Jm",
        "outputId": "23042578-0446-4fe0-d3b0-a6eba6086781"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trained csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "df['Label'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epM_6dIfMYQT",
        "outputId": "40740c08-1764-4749-e4ba-dce09c214720"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['c', 'd', 'q', '4', '1', '9', 'n', 'w', 'j', 'o', '6', 'f', 'u',\n",
              "       '3', 'z', '5', 'r', '8', 'y', '7', 'l', 's', 'k', '0', 'm', 'b',\n",
              "       'v', '2', 'x', 'h', 'p', 't', 'e', 'g', 'i', 'a'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length=len(df['Label'].unique())\n",
        "length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBju0jp3Mlhs",
        "outputId": "64b2cf35-7b06-4d60-f9b2-58ff65741452"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df,val_df=train_test_split(df,test_size=0.2,random_state=42)\n"
      ],
      "metadata": {
        "id": "XTn7vbKfNFVg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Augmentation"
      ],
      "metadata": {
        "id": "ncUoscxQ_V3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen=ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "val_datagen=ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "fmEr6JNi_IM-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='image_path',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_dataframe(dataframe=val_df,\n",
        "                                                        x_col='image_path',\n",
        "                                                        y_col='Label',\n",
        "                                                        target_size=(224, 224),\n",
        "                                                        batch_size=32,\n",
        "                                                        class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YECy4axR_97C",
        "outputId": "c0e01081-474d-41ff-e232-89e132e9a56d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2012 validated image filenames belonging to 36 classes.\n",
            "Found 503 validated image filenames belonging to 36 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition:\n",
        "\n",
        "Using : Inceptionv3\n",
        "- changing the final layer to a softmax classification layer with classes len."
      ],
      "metadata": {
        "id": "6BCfevFSFamT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import InceptionV3\n",
        "\n",
        "base_model=InceptionV3(weights='imagenet',include_top=False,input_shape=(224,224,3))"
      ],
      "metadata": {
        "id": "uxDGQ3YYBQAU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.preprocessing import image\n"
      ],
      "metadata": {
        "id": "Wj_odXa0Gj76"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(length, activation='softmax')(x)\n"
      ],
      "metadata": {
        "id": "JqgChwyVB7zE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model(inputs=base_model.input,outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "_h_etx4_GxBN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator, epochs=5,validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT2AB7eJG201",
        "outputId": "8e128a2a-2b01-49fc-dce3-29072936248a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 501ms/step - accuracy: 0.8830 - loss: 0.2852 - val_accuracy: 0.7952 - val_loss: 0.8221\n",
            "Epoch 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 488ms/step - accuracy: 0.8850 - loss: 0.3339 - val_accuracy: 0.5507 - val_loss: 1.8909\n",
            "Epoch 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 518ms/step - accuracy: 0.8865 - loss: 0.3503 - val_accuracy: 0.7197 - val_loss: 0.9304\n",
            "Epoch 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 484ms/step - accuracy: 0.9134 - loss: 0.2555 - val_accuracy: 0.8231 - val_loss: 0.5863\n",
            "Epoch 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 499ms/step - accuracy: 0.9347 - loss: 0.1954 - val_accuracy: 0.7555 - val_loss: 1.0957\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x795e995f0650>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load image and resize\n",
        "img = image.load_img(\"/content/asl_dataset/3/hand1_3_bot_seg_2_cropped.jpeg\", target_size=(224, 224))\n",
        "\n",
        "# Convert to array\n",
        "img_array = image.img_to_array(img)\n",
        "\n",
        "# Normalize\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "# Expand dims to simulate batch of 1\n",
        "img_processed = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(img_processed)\n",
        "print(x[np.argmax(prediction)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfy2LjSAIuGu",
        "outputId": "e18a013e-222a-4651-e68a-adf8b6640231"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "\n",
        "# List images in the folder\n",
        "folder_path = '/content/asl_dataset/1'\n",
        "image_files = os.listdir(folder_path)\n",
        "\n",
        "# Choose one image (e.g., the first)\n",
        "image_path =\"/content/asl_dataset/1/hand1_1_bot_seg_1_cropped.jpeg\"\n",
        "# Load and show\n",
        "img = mpimg.imread(image_path)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.title(\"Sample from class '1'\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "28fTKd5aIwGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        " 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
        " 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
        " 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
      ],
      "metadata": {
        "id": "MnBPuFZgQ3gJ"
      },
      "execution_count": 68,
      "outputs": []
    }
  ]
}